name: Automation Recon (24/7)

on:
  schedule:
    - cron: '13,33,53 * * * *' 
  workflow_dispatch:

jobs:
  recon-job:
    runs-on: ubuntu-latest
    timeout-minutes: 340
    strategy:
      fail-fast: false
      matrix:
        sniper_id: [1, 2, 3, 4, 5]
    permissions:
      contents: write
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v3

      - name: Cache Seen URLs
        uses: actions/cache@v3
        with:
          path: .seen_urls
          key: seen-urls-cache-${{ github.run_id }}
          restore-keys: |
            seen-urls-cache-

      - name: Set up Go
        uses: actions/setup-go@v4
        with:
          go-version: '>=1.20'

      - name: Install Tools
        run: |
          sudo apt-get update && sudo apt-get install -y dnsutils
          go install -v github.com/projectdiscovery/subfinder/v2/cmd/subfinder@latest
          go install -v github.com/projectdiscovery/httpx/cmd/httpx@latest
          go install -v github.com/projectdiscovery/nuclei/v3/cmd/nuclei@latest
          go install -v github.com/hakluke/hakrawler@latest
          go install -v github.com/tomnomnom/assetfinder@latest
          go install -v github.com/tomnomnom/waybackurls@latest
          go install -v github.com/lc/gau/v2/cmd/gau@latest
          go install -v github.com/tomnomnom/anew@latest
          echo "$HOME/go/bin" >> $GITHUB_PATH
          nuclei -ut -silent
          
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.9'

      - name: Install Python Dependencies
        run: pip install requests

      - name: Run Predator Recon
        env:
          GROQ_API_KEY: ${{ secrets.GROQ_API_KEY }}
          H1_USERNAME: ${{ secrets.H1_USERNAME }}
          H1_API_KEY: ${{ secrets.H1_API_KEY }}
          TG_TOKEN: ${{ secrets.TELEGRAM_TOKEN }}
          TG_CRIT: ${{ secrets.TELEGRAM_CRITICAL_ID }}
          TG_GEN: ${{ secrets.TELEGRAM_GENERAL_ID }}
          TG_DATA: ${{ secrets.TELEGRAM_DATA_ID }}
          KATANA_CHROME_PATH: /usr/bin/google-chrome
        run: |
          # 1. SETUP NOTIF CONFIG
          echo "telegram:" > notify-config.yaml
          echo "  - id: \"p1_p2_alerts\"" >> notify-config.yaml
          echo "    telegram_apikey: \"$TG_TOKEN\"" >> notify-config.yaml
          echo "    telegram_chatid: \"$TG_CRIT\"" >> notify-config.yaml
          echo "    format: \"{{data}}\"" >> notify-config.yaml

          # 2. ANTI-COLLISION DELAY
          SLEEP_TIME=$(( (${{ matrix.sniper_id }} - 1) * 40 ))
          echo "â³ Sniper ${{ matrix.sniper_id }} preparing deep-scan tools..."
          sleep $SLEEP_TIME

          # 3. TARGET SELECTION (SEQUENTIAL NO-OVERLAP)
          ls targets/*.txt | sort > targets_sorted.tmp
          TOTAL_TARGETS=$(wc -l < targets_sorted.tmp)
          RUN_NUM=${{ github.run_number }}
          SNIPER_ID=${{ matrix.sniper_id }}
          SNIPER_COUNT=5 
          
          INDEX=$(( ((RUN_NUM - 1) * SNIPER_COUNT + (SNIPER_ID - 1)) % TOTAL_TARGETS + 1 ))
          target_file=$(sed -n "${INDEX}p" targets_sorted.tmp)
          program_name=$(basename "$target_file" .txt)
          
          echo "ðŸŽ¯ Sniper $SNIPER_ID Engaging Mission #$RUN_NUM: $program_name"
          mkdir -p "data/$program_name"
          touch .seen_urls

          # STEP 1: DISCOVERY
          subfinder -dL "$target_file" -all -silent -o "data/$program_name/s1.txt" > /dev/null 2>&1 || true
          while read -r domain; do
            echo "$domain" | assetfinder --subs-only >> "data/$program_name/s1.txt" 2>/dev/null || true
          done < "$target_file"
          cat "data/$program_name/s1.txt" | sed 's/*.//g' | tr -d ' ' | sort -u > "data/$program_name/subdomains.txt"

          if [ -s "data/$program_name/subdomains.txt" ]; then
            # STEP 2: HTTPX
            timeout 20m httpx -l "data/$program_name/subdomains.txt" -silent -ip -td -mc 200,301,302,401,403,405 -H "User-Agent: NovaRecon/2026" -o "data/$program_name/active_hosts.txt" > /dev/null 2>&1 || true
            
            if [ -s "data/$program_name/active_hosts.txt" ]; then
                # --- [STEP 3: DEEP SIGHT 3.0 - CLEAN VERSION] ---
                echo "[+] Step 3: Mining Endpoints..."
                # Ambil hanya kolom pertama (URL murni) dari active_hosts agar crawler gak bingung
                awk '{print $1}' data/$program_name/active_hosts.txt > data/$program_name/active_urls_clean.tmp
                
                # Bersihkan domain untuk Wayback/GAU
                sed 's|http[s]*://||g' data/$program_name/active_urls_clean.tmp | cut -d' ' -f1 | sort -u > data/$program_name/domains_only.txt
                
                timeout 10m bash -c "cat data/$program_name/domains_only.txt | waybackurls > data/$program_name/urls_wayback.txt" || true
                timeout 10m bash -c "cat data/$program_name/domains_only.txt | gau --subs > data/$program_name/urls_gau.txt" || true
                
                echo "[+] Step 4: (Crawling with Hakrawler...)"
                # Gunakan URL yang sudah bersih tadi
                timeout 15m bash -c "cat data/$program_name/active_urls_clean.tmp | hakrawler -subs > data/$program_name/urls_hak.txt" || true

                # Gabungkan SEMUA temuan
                touch data/$program_name/urls_wayback.txt data/$program_name/urls_gau.txt data/$program_name/urls_hak.txt
                cat data/$program_name/urls_wayback.txt data/$program_name/urls_gau.txt data/$program_name/urls_hak.txt | sort -u > data/$program_name/all_endpoints.txt
                
                # Fallback: Jika zonk, tetap pakai active_hosts
                if [ ! -s "data/$program_name/all_endpoints.txt" ]; then
                  cp data/$program_name/active_urls_clean.tmp data/$program_name/all_endpoints.txt
                fi
                
                echo "    âœ… Done. Total targets for Nuclei: $(wc -l < data/$program_name/all_endpoints.txt)"

                # --- [STEP 4: NUCLEI KILLSHOT] ---
                echo "[+] Phase 1: Scanning Gold (P1-P3)..."
                # Kita tembak data all_endpoints yang isinya ribuan tadi!
                timeout 50m nuclei -l "data/$program_name/all_endpoints.txt" \
                  -severity critical,high,medium \
                  -et dns,ssl,whois -es info \
                  -rl 150 -c 70 -mhe 100 -no-color -silent \
                  -H "User-Agent: NovaRecon/2026" \
                  -jsonl -o "data/$program_name/nuclei_results.json" || true

                # STEP 5: TOKEN SCAN
                timeout 15m nuclei -l "data/$program_name/all_endpoints.txt" -t http/exposed-tokens/ -H "User-Agent: NovaRecon/2026" -rl 80 -no-color -mhe 100 -silent -jsonl -o "data/$program_name/token_results.json" > /dev/null 2>&1 || true
                [ -f "data/$program_name/token_results.json" ] && cat "data/$program_name/token_results.json" >> "data/$program_name/nuclei_results.json"

                # STEP 6: AI ANALYSIS
                echo "[+] Step 4: AI Analysis & H1 Drafting..."
                PROGRAM_NAME=$program_name python scripts/validate.py > /dev/null 2>&1 || true

                # STEP 7: TELEGRAM NOTIF & BACKUP
                if [ -d "data/$program_name/alerts/high" ]; then
                  for f in data/$program_name/alerts/high/*.md; do
                    [ -e "$f" ] || continue
                    MSG_HIGH=$(cat "$f")
                    curl -s -X POST "https://api.telegram.org/bot$TG_TOKEN/sendMessage" -d "chat_id=$TG_CRIT" -d "text=$MSG_HIGH" > /dev/null 2>&1
                  done
                fi

                zip -qj "${program_name}_FULL_DATA.zip" data/$program_name/*.txt data/$program_name/*.json .seen_urls || true
                curl -s -F chat_id="$TG_DATA" -F document=@"${program_name}_FULL_DATA.zip" -F caption="ðŸ“¦ DATABASE: $program_name" "https://api.telegram.org/bot$TG_TOKEN/sendDocument" > /dev/null 2>&1
            fi
          fi

      - name: Sync Database to Repo
        run: |
          git config --local user.email "bot@sniper-recon.ai"
          git config --local user.name "Recon-Sniper-Bot"
          git add .seen_urls
          git commit -m "database: update seen_urls [skip ci]" || exit 0
          # FIX: Gunakan --rebase agar 5 sniper tidak saling tabrak saat push
          git pull --rebase origin main
          git push
